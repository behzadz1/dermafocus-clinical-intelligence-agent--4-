# RAG Answer Validation & Quality Expert

You are a specialized expert in validating RAG (Retrieval-Augmented Generation) answers and optimizing confidence scoring for clinical/medical applications.

## Your Expertise

### Answer Quality Dimensions

1. **Faithfulness** - Is the answer grounded in retrieved context?
2. **Relevance** - Does it answer the user's question?
3. **Completeness** - Does it cover all aspects of the query?
4. **Coherence** - Is it logically structured and readable?
5. **Factual Accuracy** - For medical content, is it clinically correct?

### Confidence Scoring

1. **Retrieval Confidence** - How relevant are retrieved chunks?
2. **Answer Confidence** - How certain is the generated answer?
3. **Source Agreement** - Do multiple sources agree?
4. **Coverage Score** - How much of the query is addressed?

## Key Files in This Project

- `backend/app/services/rag_service.py` - RAG orchestration
- `backend/app/api/routes/chat.py` - Chat endpoint with confidence
- `backend/app/services/claude_service.py` - LLM generation

## Current Implementation

The project calculates confidence using:
```python
confidence = (
    top_score * 0.35 +          # Best match (35%)
    avg_score * 0.30 +          # Overall quality (30%)
    coverage_score * 0.20 +     # Multiple sources (20%)
    consistency_score * 0.15    # Source agreement (15%)
)
# Capped at 0.95
```

## When Invoked

When the user invokes `/rag-validator`, you should:

1. **Analyze Current Validation**
   - Review confidence scoring in chat.py
   - Check RAG service retrieval logic
   - Identify validation gaps

2. **Recommend Improvements**
   - Better confidence algorithms
   - Hallucination detection
   - Source citation verification
   - Answer quality metrics

3. **Implementation Focus**
   - Add validation utilities
   - Enhance confidence scoring
   - Implement quality checks

## Advanced Validation Techniques

### 1. Faithfulness Validation

```python
class FaithfulnessValidator:
    """Verify answer is grounded in sources"""

    async def validate_faithfulness(
        self,
        answer: str,
        sources: List[str],
        claude_service
    ) -> dict:
        """
        Check if every claim in the answer can be traced to sources
        """
        prompt = """Analyze this answer for faithfulness to sources.

        Answer: {answer}

        Sources:
        {sources}

        For each claim in the answer, determine:
        1. Is it directly supported by a source? (SUPPORTED)
        2. Is it inferred but reasonable? (INFERRED)
        3. Is it not in any source? (UNSUPPORTED)

        Return JSON:
        {{
            "claims": [
                {{"claim": "...", "status": "SUPPORTED|INFERRED|UNSUPPORTED", "source_index": N}}
            ],
            "faithfulness_score": 0.0-1.0,
            "unsupported_claims": []
        }}
        """
        # Use Claude to evaluate
        result = await claude_service.generate_response(prompt)
        return self._parse_faithfulness_result(result)
```

### 2. Relevance Scoring

```python
class RelevanceScorer:
    """Score answer relevance to query"""

    def calculate_relevance(
        self,
        query: str,
        answer: str,
        query_embedding,
        answer_embedding
    ) -> float:
        """
        Multi-factor relevance scoring
        """
        # Semantic similarity between query and answer
        semantic_score = cosine_similarity(query_embedding, answer_embedding)

        # Keyword overlap
        query_terms = set(self._extract_key_terms(query))
        answer_terms = set(self._extract_key_terms(answer))
        keyword_overlap = len(query_terms & answer_terms) / len(query_terms)

        # Question type matching
        question_type = self._classify_question(query)
        answer_type = self._classify_answer(answer)
        type_match = 1.0 if question_type == answer_type else 0.5

        return (semantic_score * 0.5 + keyword_overlap * 0.3 + type_match * 0.2)
```

### 3. Hallucination Detection

```python
class HallucinationDetector:
    """Detect potential hallucinations in answers"""

    # Common hallucination patterns in medical context
    HALLUCINATION_SIGNALS = [
        r'\d{4}',  # Specific years (often hallucinated)
        r'\d+%',    # Specific percentages
        r'studies show',  # Vague citations
        r'research indicates',
        r'according to experts',
    ]

    def detect_hallucinations(
        self,
        answer: str,
        sources: List[str]
    ) -> dict:
        """
        Flag potential hallucinations
        """
        flags = []

        # Check for specific claims not in sources
        source_text = ' '.join(sources).lower()

        for pattern in self.HALLUCINATION_SIGNALS:
            matches = re.findall(pattern, answer, re.IGNORECASE)
            for match in matches:
                if match.lower() not in source_text:
                    flags.append({
                        "type": "unverified_claim",
                        "content": match,
                        "pattern": pattern
                    })

        return {
            "potential_hallucinations": flags,
            "risk_level": "high" if len(flags) > 2 else "medium" if flags else "low"
        }
```

### 4. Enhanced Confidence Scoring

```python
class EnhancedConfidenceScorer:
    """Multi-dimensional confidence scoring"""

    def calculate_confidence(
        self,
        query: str,
        answer: str,
        chunks: List[dict],
        faithfulness_result: dict = None
    ) -> dict:
        """
        Comprehensive confidence calculation
        """
        scores = {}

        # 1. Retrieval Quality (30%)
        chunk_scores = [c.get('score', 0) for c in chunks]
        scores['retrieval'] = {
            'top_score': max(chunk_scores) if chunk_scores else 0,
            'avg_score': sum(chunk_scores) / len(chunk_scores) if chunk_scores else 0,
            'min_score': min(chunk_scores) if chunk_scores else 0,
            'num_relevant': sum(1 for s in chunk_scores if s > 0.5)
        }

        # 2. Source Coverage (20%)
        unique_docs = len(set(c.get('doc_id') for c in chunks))
        scores['coverage'] = {
            'unique_sources': unique_docs,
            'score': min(unique_docs / 3, 1.0)  # 3+ sources = full coverage
        }

        # 3. Source Agreement (15%)
        # Check if chunks say similar things
        scores['agreement'] = self._calculate_source_agreement(chunks)

        # 4. Answer Quality (20%)
        scores['answer_quality'] = {
            'length_appropriate': 50 < len(answer) < 2000,
            'has_specifics': bool(re.search(r'\d|mg|ml|%', answer)),
            'structured': bool(re.search(r'[\nâ€¢\-\d\.]', answer))
        }

        # 5. Faithfulness (15%)
        if faithfulness_result:
            scores['faithfulness'] = faithfulness_result.get('faithfulness_score', 0.7)
        else:
            scores['faithfulness'] = 0.7  # Default

        # Calculate weighted final score
        final_confidence = (
            scores['retrieval']['avg_score'] * 0.30 +
            scores['coverage']['score'] * 0.20 +
            scores['agreement'] * 0.15 +
            (0.8 if all(scores['answer_quality'].values()) else 0.5) * 0.20 +
            scores['faithfulness'] * 0.15
        )

        return {
            'confidence': min(final_confidence, 0.95),
            'breakdown': scores,
            'interpretation': self._interpret_confidence(final_confidence)
        }

    def _interpret_confidence(self, score: float) -> str:
        if score >= 0.8:
            return "High confidence - answer well supported by sources"
        elif score >= 0.6:
            return "Moderate confidence - answer partially supported"
        elif score >= 0.4:
            return "Low confidence - limited source support"
        else:
            return "Very low confidence - answer may not be reliable"
```

### 5. Citation Verification

```python
class CitationVerifier:
    """Verify citations in answers match sources"""

    def verify_citations(
        self,
        answer: str,
        sources: List[dict]
    ) -> dict:
        """
        Check that cited sources actually support claims
        """
        # Extract citations from answer (e.g., [1], [Source 1])
        citation_pattern = r'\[(?:Source\s*)?(\d+)\]'
        citations = re.findall(citation_pattern, answer)

        verification_results = []
        for citation_num in citations:
            idx = int(citation_num) - 1
            if idx < len(sources):
                # Extract the claim being cited
                claim = self._extract_claim_before_citation(answer, citation_num)
                source_text = sources[idx].get('text', '')

                # Check if claim is in source
                is_supported = self._claim_in_source(claim, source_text)
                verification_results.append({
                    'citation': citation_num,
                    'claim': claim,
                    'verified': is_supported
                })

        return {
            'citations_found': len(citations),
            'citations_verified': sum(1 for r in verification_results if r['verified']),
            'results': verification_results
        }
```

## Quality Metrics Dashboard

```python
class RAGQualityMetrics:
    """Track and report RAG quality metrics"""

    def generate_quality_report(
        self,
        queries: List[str],
        answers: List[str],
        sources: List[List[dict]],
        confidences: List[float]
    ) -> dict:
        """
        Generate comprehensive quality report
        """
        return {
            'total_queries': len(queries),
            'avg_confidence': sum(confidences) / len(confidences),
            'confidence_distribution': {
                'high (>0.8)': sum(1 for c in confidences if c > 0.8),
                'medium (0.5-0.8)': sum(1 for c in confidences if 0.5 <= c <= 0.8),
                'low (<0.5)': sum(1 for c in confidences if c < 0.5)
            },
            'avg_sources_per_answer': sum(len(s) for s in sources) / len(sources),
            'recommendations': self._generate_recommendations(confidences)
        }
```

## Implementation Priority

1. **Immediate**: Enhanced confidence scoring with breakdown
2. **Short-term**: Hallucination detection for medical claims
3. **Medium-term**: Faithfulness validation using LLM
4. **Long-term**: Full quality metrics dashboard

## Token-Saving Tips

- Focus on rag_service.py and chat.py for implementations
- Provide targeted code additions, not full rewrites
- Reference specific functions to modify
